---
title: "Modelado bayesiano en R con {rstanarm}"
subtitle: "Tutorial useR!21"  
author: 
  - "Fernando A. Zepeda Herrera"
date: '7 de julio de 2021'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.retina = 3, warning = FALSE, message = FALSE)
library(emojifont)
library(tidyverse)
library(icons)
load.fontawesome()
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#44546A",
  header_font_google = google_font("Noto Serif"),
  text_font_google   = google_font("Montserrat"),
  code_font_google   = google_font("Fira Mono")
)
```

```{r colorize, include=FALSE, warning=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

.center[
## ¡Bienvenidos!
```{r echo=FALSE, out.width="50%"}
knitr::include_graphics("img/mascot_useR21.png")
```
]

---
class: left

# Algo sobre mí

- Mini CV para el final

---
class: left

# Algo sobre ustedes

- Pre requisitos 

---
class: left

# Algo sobre hoy

- Paradigma bayesiano

- Una forma de pensar en regresiones

- Un pequeño gran problema

- Imaginar una solución

- Manos a la obra

---
class: inverse, center, middle

# Empecemos

---
class: inverse, center, middle

# Paradigma bayesiano

---

# ¿Qué es la probabilidad?

- En matemáticas algo informales podemos hablar de una función no negativa que suma a 1. 

- Pero pensemos en la última vez que usamos el término probabilidad. 

--

.center[
¿Cuál es la *probabilidad* de que esté lloviendo afuera de mi ventana?

![Un gif de lluvia](https://media.giphy.com/media/gg3AmWYDL0hb2/giphy.gif)

]


---

# ¿Qué es la probabilidad?

- En matemáticas algo informales podemos hablar de una función no negativa que suma a 1. 

- Pero pensemos en la última vez que usamos el término probabilidad. 

.center[

Lo más *probable* es que mi equipo gane. 

![Don Ramón y Necaxa Fuente: Radio Futuro Chile](https://www.futuro.cl/wp-content/uploads/2019/08/don-ramon-necaxa-web-590x340.jpg)
]

---

# ¿Qué es la probabilidad?

- En matemáticas algo informales podemos hablar de una función no negativa que suma a 1. 

- Pero pensemos en la última vez que usamos el término probabilidad. 

.center[
**¿Otros ejemplos?**

```{r echo=FALSE, out.width="30%"}
knitr::include_graphics("img/mascot_useR21.png")
```
]

---

# ¿Qué es la probabilidad?

- Interpretaciones clásica o frecuentista más enfocadas en verla como una *medida de variabilidad*. 

- La interpretación bayesiana es que es una **medida de incertidumbre**. 

.center[
```{r echo=FALSE, out.width="30%"}
knitr::include_graphics("https://media.giphy.com/media/xUOxfjsW9fWPqEWouI/giphy.gif")
```
]

- Por ello podemos asignar distribuciones de probabilidad a todas aquellas cantidades que desconocemos.

---

# La receta 

El paradigma bayesiano entonces, al interpretar la probabilidad como medida de incertidumbre, en la práctica tiene, lo que **Gutiérrez Peña (2016)** la receta de la inferencia bayesiana: 

> ... encontrar la distribución condicional de todas aquellas cantidades de interés cuyo valor desconocemos dado el valor conocido de las variables observadas.


---

# Un propósito de inversión

Pensemos en esta explicación de **Christian Robert (2007)**

> ... el propósito de un análisis estadístico es fundamentalmente un propósito de inversión, puesto que busca recuperar las causas— reducidas a los parámetros del mecanismo probabilístico de generación— de los efectos— resumidos por las observaciones. En otras palabras, al observar un fenómeno aleatorio dirigido por un parámetro $\theta$, los métodos estadísticos permiten deducir de estas observaciones una inferencia (esto es, un resumen, una caracterización) sobre $\theta$ [...] La inferencia está basada entonces en la distribución de $\theta$ condicional en $x$ [...] llamada la distribución posterior...

---

# Teorema de Bayes

4 componentes básicos: 

.pull-left[
- **`r colorize("Posterior", "#44546A")`**
]

.pull-right[
$$\color{#44546A}{p(\theta | x)} = \dfrac{p(x | \theta) p(\theta)}{p(x)}$$
]

<br>
<br>
<br>

<hr>
La distribución **`r colorize("posterior", "#44546A")`** es nuestro objetivo. 

Recordemos la receta: queremos la distribución de aquello que desconocemos, el parámetro, dados los datos observados. 

---

# Teorema de Bayes

4 componentes básicos: 

.pull-left[
- **`r colorize("Posterior", "#44546A")`**
- **`r colorize("Inicial", "#F0A868")`**
]

.pull-right[
$$\color{#44546A}{p(\theta | x)} = \dfrac{p(x | \theta) \color{#F0A868}{p(\theta)}}{p(x)}$$
]


<br>
<br>
<br>

<hr>
La distribución **`r colorize("inicial", "#F0A868")`**, también conocida como *`r colorize("a priori", "#F0A868")`*, es con lo que empezamos. 

¿Qué sabemos sobre el parámetro *antes de observar los datos*? 

---

# Teorema de Bayes

4 componentes básicos: 

.pull-left[
- **`r colorize("Posterior", "#44546A")`**
- **`r colorize("Inicial", "#F0A868")`**
- **`r colorize("Verosimilitud", "#87BBA2")`**
]

.pull-right[
$$\color{#44546A}{p(\theta | x)} = \dfrac{\color{#87BBA2}{p(x | \theta)} \color{#F0A868}{p(\theta)}}{p(x)}$$
]


<br>
<br>
<br>

<hr>
La función de **`r colorize("verosimilitud", "#87BBA2")`** es la distribución condicional de los datos dado el parámetro, pero vista como función del parámetro y con los datos fijos. 

Es la misma función utilizada para encontrar los estimadores de máxima verosimilitud en un contexto frecuentista. 

---

# Teorema de Bayes

4 componentes básicos: 

.pull-left[
- **`r colorize("Posterior", "#44546A")`**
- **`r colorize("Inicial", "#F0A868")`**
- **`r colorize("Verosimilitud", "#87BBA2")`**
- **`r colorize("Marginal", "#F45B69")`**
]

.pull-right[
$$\color{#44546A}{p(\theta | x)} = \dfrac{\color{#87BBA2}{p(x | \theta)} \color{#F0A868}{p(\theta)}}{\color{#F45B69}{p(x)}}$$
]

<br>
<br>
<br>

<hr>
La distribución **`r colorize("marginal", "#F45B69")`** de los datos. Podemos pensar en ella como un promedio ponderado de las distintas verosimilitudes a través de los posibles valores del parámetro: 

$${\color{#F45B69}{p(x)}} = \sum\limits_{i=1}^k p(x | \theta_i) p(\theta_i) \qquad\text{o}\qquad{\color{#F45B69}{p(x)}} = \int\limits_{\Theta} p(x | \theta) p(\theta) d\theta$$

---

# Un ejemplo médico

Supongamos que estamos interesados en saber si un tratamiento médico es efectivo o no contra alguna enfermedad. 

- Se tienen $N$ voluntarios en un ensayo clínico y se busca estimar la efectividad $\theta$, definida como el porcentaje de pacientes que responden satisfactoriamente al tratamiento. 

--

- Por ejemplo, si una persona responde al tratamiento, lo representamos como `r icon_style(fontawesome("heartbeat", style = "solid"), fill = "#87BBA2")`. 

--

- Si alguien *no* responde al tratamiento utilizamos otro color: `r icon_style(fontawesome("heartbeat", style = "solid"), fill = "#F0A868")`.

--

```{r corazones_pacientes, echo=FALSE, include=FALSE}
# Y luego copiar y pegar en PPT
set.seed(417526)
pacientes <- tibble(Paciente = 1:30, Resultado = rbinom(30,1,0.65)) %>% 
  mutate(Icono = emojifont::fontawesome("fa-heartbeat")) 

pacientes %>%
  ggplot(aes(x=Paciente,colour = as.factor(Resultado))) + 
  geom_text(aes(y=1, label = Icono),  family = "fontawesome-webfont", 
            show.legend = FALSE, size = rel(5.5))  + 
  scale_color_manual(values = c("#F0A868","#87BBA2")) +
  coord_cartesian(xlim = c(0, 31), ylim = c(1,1), expand = FALSE) + 
  theme_void()
```
Supongamos que $N=30$ y los pacientes tuvieron los siguientes resultados:
.center[
```{r echo=FALSE, out.width="90%"}
knitr::include_graphics("img/pacientes_corazones.png")
```
]

--

Tenemos entonces `r sum(pacientes$Resultado)` éxitos del tratamiento contra `r nrow(pacientes) - sum(pacientes$Resultado)` fracasos. 