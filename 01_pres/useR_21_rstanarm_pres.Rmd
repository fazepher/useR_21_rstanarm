---
title: "Modelado bayesiano en R con {rstanarm}"
subtitle: "Tutorial useR!21"  
author: 
  - "Fernando A. Zepeda Herrera"
date: '7 de julio de 2021'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#44546A",
  header_font_google = google_font("Noto Serif"),
  text_font_google   = google_font("Montserrat"),
  code_font_google   = google_font("Fira Mono")
)
```

```{r colorize, include=FALSE, warning=FALSE}
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}
```

.center[
## ¡Bienvenidos!
```{r echo=FALSE, out.width="50%", fig.cap="useR!21 Conference Mascot"}
knitr::include_graphics("img/mascot_useR21.png")
```
]

---
class: left

# Algo sobre mí

- Mini CV para el final

---
class: left

# Algo sobre ustedes

- Pre requisitos 

---
class: left

# Algo sobre hoy

- Paradigma bayesiano

- Una forma de pensar en regresiones

- Un pequeño gran problema

- Imaginar una solución

- Manos a la obra

---
class: inverse, center, middle

# Empecemos

---
class: inverse, center, middle

# Paradigma bayesiano

---

# ¿Qué es la probabilidad?

En matemáticas algo informales podemos hablar de una función no negativa que suma a 1. 

Pero pensemos en la última vez que usamos el término probabilidad. 

¿Cuál es la *probabilidad* de que esté lloviendo afuera de mi ventana?

Lo más *probable* es que el equipo A gane. 

¿Algunos otros ejemplos?

---

# ¿Qué es la probabilidad?

Interpretaciones clásica o frecuentista más enfocadas en verla como una medida de *variabilidad*. 

La interpretación bayesiana es que es una medida de incertidumbre. 

**citar, por ejemplo, Mendoza, Bernardo, Gelman, Singurpuwalla, mi tesis**

Por ello podemos asignar distribuciones de probabilidad a todas aquellas cantidades que desconocemos.

---

# La receta 

El paradigma bayesiano entonces, al interpretar la probabilidad como medida de incertidumbre, en la práctica tiene, lo que **Gutiérrez Peña (2016)** la receta de la inferencia bayesiana: 

> ... encontrar la distribución condicional de todas aquellas cantidades de interés cuyo valor desconocemos dado el valor conocido de las variables observadas.


---

# Un propósito de inversión

Pensemos en esta explicación de **Christian Robert (2007)**

> ... el propósito de un análisis estadístico es fundamentalmente un propósito de inversión, puesto que busca recuperar las causas— reducidas a los parámetros del mecanismo probabilístico de generación— de los efectos— resumidos por las observaciones. En otras palabras, al observar un fenómeno aleatorio dirigido por un parámetro $\theta$, los métodos estadísticos permiten deducir de estas observaciones una inferencia (esto es, un resumen, una caracterización) sobre $\theta$ [...] La inferencia está basada entonces en la distribución de $\theta$ condicional en $x$ [...] llamada la distribución posterior...

---

# Teorema de Bayes

4 componentes básicos: 

.pull-left[
- **`r colorize("Posterior", "#44546A")`**
]

.pull-right[
$$\color{#44546A}{p(\theta | x)} = \dfrac{p(x | \theta) p(\theta)}{p(x)}$$
]

<br>
<br>
<br>

<hr>
La distribución **`r colorize("posterior", "#44546A")`** es nuestro objetivo. 

Recordemos la receta: queremos la distribución de aquello que desconocemos, el parámetro, dados los datos observados. 

---

# Teorema de Bayes

4 componentes básicos: 

.pull-left[
- **`r colorize("Posterior", "#44546A")`**
- **`r colorize("Inicial", "#F0A868")`**
]

.pull-right[
$$\color{#44546A}{p(\theta | x)} = \dfrac{p(x | \theta) \color{#F0A868}{p(\theta)}}{p(x)}$$
]


<br>
<br>
<br>

<hr>
La distribución **`r colorize("inicial", "#F0A868")`**, también conocida como *`r colorize("a priori", "#F0A868")`*, es con lo que empezamos. 

¿Qué sabemos sobre el parámetro *antes de observar los datos*? 

---

# Teorema de Bayes

4 componentes básicos: 

.pull-left[
- **`r colorize("Posterior", "#44546A")`**
- **`r colorize("Inicial", "#F0A868")`**
- **`r colorize("Verosimilitud", "#87BBA2")`**
]

.pull-right[
$$\color{#44546A}{p(\theta | x)} = \dfrac{\color{#87BBA2}{p(x | \theta)} \color{#F0A868}{p(\theta)}}{p(x)}$$
]


<br>
<br>
<br>

<hr>
La función de **`r colorize("verosimilitud", "#87BBA2")`** es la distribución condicional de los datos dado el parámetro, pero vista como función del parámetro y con los datos fijos. 

Es la misma función utilizada para encontrar los estimadores de máxima verosimilitud en un contexto frecuentista. 

---

# Teorema de Bayes

4 componentes básicos: 

.pull-left[
- **`r colorize("Posterior", "#44546A")`**
- **`r colorize("Inicial", "#F0A868")`**
- **`r colorize("Verosimilitud", "#87BBA2")`**
- **`r colorize("Marginal", "#F45B69")`**
]

.pull-right[
$$\color{#44546A}{p(\theta | x)} = \dfrac{\color{#87BBA2}{p(x | \theta)} \color{#F0A868}{p(\theta)}}{\color{#F45B69}{p(x)}}$$
]

<br>
<br>
<br>

<hr>
La distribución **`r colorize("marginal", "#F45B69")`** de los datos. Podemos pensar en ella como un promedio ponderado de las distintas verosimilitudes a través de los posibles valores del parámetro: 

$${\color{#F45B69}{p(x)}} = \sum\limits_{i=1}^k p(x | \theta_i) p(\theta_i) \qquad\text{o}\qquad{\color{#F45B69}{p(x)}} = \int\limits_{\Theta} p(x | \theta) p(\theta) d\theta$$

